{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python workspace\\Machine_Leaning\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['img', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['img', 'label'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "data = load_dataset(\"uoft-cs/cifar10\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data['train']\n",
    "test_data = data['test']\n",
    "# Shuffling the training data to make sure the model does not learn the order of the data\n",
    "train_data = train_data.shuffle(seed=42)\n",
    "train_data = train_data[:]\n",
    "test_data = test_data[:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "# Converting the PIL type images to tensors\n",
    "\n",
    "from torchvision import transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "# Checking the length of the training data so I can loop through it and transofrm every PIL image to a tensor whic helps me to train the model\n",
    "print(len(train_data['img']))\n",
    "for i in range(len(train_data['img'])):\n",
    "    train_data['img'][i] = transform(train_data['img'][i])\n",
    "for i in range(len(test_data['img'])):\n",
    "    test_data['img'][i] = transform(test_data['img'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[0.1255, 0.1255, 0.1294,  ..., 0.8039, 0.6196, 0.6471],\n",
       "          [0.1333, 0.1451, 0.1451,  ..., 0.7843, 0.6039, 0.6078],\n",
       "          [0.1529, 0.1608, 0.1608,  ..., 0.6980, 0.5725, 0.5569],\n",
       "          ...,\n",
       "          [0.2275, 0.2275, 0.2275,  ..., 0.4863, 0.5059, 0.5333],\n",
       "          [0.2275, 0.2275, 0.2314,  ..., 0.4863, 0.5020, 0.5216],\n",
       "          [0.2275, 0.2235, 0.2353,  ..., 0.4863, 0.4941, 0.5098]],\n",
       " \n",
       "         [[0.1098, 0.0980, 0.0941,  ..., 0.6824, 0.5529, 0.5765],\n",
       "          [0.1255, 0.1137, 0.1098,  ..., 0.6392, 0.5373, 0.5529],\n",
       "          [0.1412, 0.1255, 0.1216,  ..., 0.5647, 0.4941, 0.5020],\n",
       "          ...,\n",
       "          [0.1804, 0.1765, 0.1765,  ..., 0.4039, 0.4235, 0.4471],\n",
       "          [0.1804, 0.1804, 0.1765,  ..., 0.4039, 0.4196, 0.4392],\n",
       "          [0.1765, 0.1725, 0.1804,  ..., 0.4000, 0.4157, 0.4275]],\n",
       " \n",
       "         [[0.0980, 0.1059, 0.1137,  ..., 0.4471, 0.2902, 0.3333],\n",
       "          [0.1098, 0.1216, 0.1294,  ..., 0.3765, 0.2706, 0.2941],\n",
       "          [0.1294, 0.1373, 0.1412,  ..., 0.2980, 0.2471, 0.2667],\n",
       "          ...,\n",
       "          [0.1333, 0.1294, 0.1255,  ..., 0.2980, 0.3176, 0.3451],\n",
       "          [0.1333, 0.1294, 0.1255,  ..., 0.2980, 0.3098, 0.3373],\n",
       "          [0.1294, 0.1255, 0.1255,  ..., 0.2980, 0.3059, 0.3255]]]),\n",
       " tensor([[[0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0078,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0078, 0.0039,  ..., 0.0000, 0.0000, 0.0039],\n",
       "          ...,\n",
       "          [0.8549, 0.8157, 0.8902,  ..., 0.4078, 0.3490, 0.3255],\n",
       "          [0.8431, 0.8196, 0.8353,  ..., 0.7294, 0.3020, 0.3176],\n",
       "          [0.8000, 0.8235, 0.8196,  ..., 0.9804, 0.6196, 0.2549]],\n",
       " \n",
       "         [[0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0000, 0.0039, 0.0039],\n",
       "          ...,\n",
       "          [0.4667, 0.3490, 0.3882,  ..., 0.1608, 0.1020, 0.1098],\n",
       "          [0.4745, 0.3961, 0.3216,  ..., 0.4549, 0.1059, 0.1373],\n",
       "          [0.4275, 0.4314, 0.3490,  ..., 0.6471, 0.3765, 0.0980]],\n",
       " \n",
       "         [[0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0039,  ..., 0.0078, 0.0039, 0.0039],\n",
       "          [0.0039, 0.0039, 0.0078,  ..., 0.0118, 0.0078, 0.0039],\n",
       "          ...,\n",
       "          [0.3647, 0.2314, 0.2157,  ..., 0.0824, 0.0353, 0.0667],\n",
       "          [0.3686, 0.3020, 0.1686,  ..., 0.4000, 0.0588, 0.0510],\n",
       "          [0.3059, 0.3294, 0.2275,  ..., 0.5490, 0.3176, 0.0549]]]),\n",
       " tensor([[[0.5451, 0.5686, 0.5804,  ..., 0.4784, 0.4588, 0.4275],\n",
       "          [0.5882, 0.6078, 0.6235,  ..., 0.5098, 0.4863, 0.4706],\n",
       "          [0.6118, 0.6275, 0.6431,  ..., 0.5255, 0.5137, 0.5059],\n",
       "          ...,\n",
       "          [0.2039, 0.2000, 0.3294,  ..., 0.7725, 0.7725, 0.7686],\n",
       "          [0.2980, 0.2745, 0.2863,  ..., 0.8039, 0.8039, 0.8039],\n",
       "          [0.3373, 0.3686, 0.4314,  ..., 0.8039, 0.8039, 0.8078]],\n",
       " \n",
       "         [[0.5608, 0.5804, 0.5922,  ..., 0.5451, 0.5294, 0.4980],\n",
       "          [0.5922, 0.6118, 0.6235,  ..., 0.5686, 0.5490, 0.5333],\n",
       "          [0.6078, 0.6235, 0.6314,  ..., 0.5725, 0.5647, 0.5569],\n",
       "          ...,\n",
       "          [0.1412, 0.1529, 0.2784,  ..., 0.6980, 0.6980, 0.6941],\n",
       "          [0.2118, 0.2078, 0.2157,  ..., 0.7294, 0.7294, 0.7294],\n",
       "          [0.2353, 0.2745, 0.3333,  ..., 0.7255, 0.7255, 0.7294]],\n",
       " \n",
       "         [[0.3098, 0.3529, 0.3843,  ..., 0.3098, 0.2941, 0.2667],\n",
       "          [0.3529, 0.3922, 0.4275,  ..., 0.3333, 0.3176, 0.2980],\n",
       "          [0.3882, 0.4275, 0.4588,  ..., 0.3451, 0.3373, 0.3255],\n",
       "          ...,\n",
       "          [0.0667, 0.1020, 0.2000,  ..., 0.5373, 0.5373, 0.5373],\n",
       "          [0.1216, 0.1294, 0.1137,  ..., 0.5765, 0.5765, 0.5765],\n",
       "          [0.1176, 0.1686, 0.2078,  ..., 0.5843, 0.5843, 0.5843]]]),\n",
       " tensor([[[0.8941, 0.8667, 0.8824,  ..., 0.9294, 0.9294, 0.9294],\n",
       "          [0.8078, 0.7255, 0.7647,  ..., 0.9412, 0.9412, 0.9412],\n",
       "          [0.5412, 0.4745, 0.5176,  ..., 0.9373, 0.9412, 0.9412],\n",
       "          ...,\n",
       "          [0.3373, 0.3647, 0.3412,  ..., 0.5529, 0.4706, 0.4863],\n",
       "          [0.3294, 0.3059, 0.2745,  ..., 0.5020, 0.4118, 0.4157],\n",
       "          [0.3255, 0.3059, 0.3059,  ..., 0.3804, 0.3765, 0.3294]],\n",
       " \n",
       "         [[0.9176, 0.8941, 0.9137,  ..., 0.9373, 0.9373, 0.9373],\n",
       "          [0.8392, 0.7569, 0.7961,  ..., 0.9490, 0.9490, 0.9490],\n",
       "          [0.5843, 0.5059, 0.5451,  ..., 0.9451, 0.9490, 0.9490],\n",
       "          ...,\n",
       "          [0.3922, 0.4157, 0.3922,  ..., 0.3451, 0.3961, 0.4980],\n",
       "          [0.4078, 0.3843, 0.3529,  ..., 0.3569, 0.3882, 0.4706],\n",
       "          [0.4118, 0.3882, 0.3882,  ..., 0.4000, 0.4157, 0.3804]],\n",
       " \n",
       "         [[0.8784, 0.8588, 0.8745,  ..., 0.9333, 0.9333, 0.9333],\n",
       "          [0.7373, 0.6510, 0.6941,  ..., 0.9412, 0.9412, 0.9412],\n",
       "          [0.4157, 0.3216, 0.3765,  ..., 0.9294, 0.9333, 0.9294],\n",
       "          ...,\n",
       "          [0.1804, 0.2078, 0.1804,  ..., 0.1725, 0.2196, 0.3333],\n",
       "          [0.1961, 0.1725, 0.1451,  ..., 0.1490, 0.1804, 0.2902],\n",
       "          [0.2353, 0.2157, 0.2157,  ..., 0.1804, 0.2196, 0.2196]]]),\n",
       " tensor([[[0.6118, 0.6667, 0.7529,  ..., 0.8078, 0.7725, 0.6039],\n",
       "          [0.4980, 0.5098, 0.5922,  ..., 0.8392, 0.7373, 0.5020],\n",
       "          [0.4118, 0.4392, 0.5412,  ..., 0.8353, 0.6745, 0.4157],\n",
       "          ...,\n",
       "          [0.7882, 0.7804, 0.7647,  ..., 0.5137, 0.5294, 0.5412],\n",
       "          [0.7961, 0.7843, 0.7922,  ..., 0.5137, 0.5294, 0.5451],\n",
       "          [0.8000, 0.7804, 0.7843,  ..., 0.5569, 0.5569, 0.5529]],\n",
       " \n",
       "         [[0.6314, 0.6863, 0.7686,  ..., 0.7569, 0.7608, 0.6314],\n",
       "          [0.5216, 0.5294, 0.6118,  ..., 0.7843, 0.7255, 0.5412],\n",
       "          [0.4392, 0.4627, 0.5647,  ..., 0.7647, 0.6549, 0.4510],\n",
       "          ...,\n",
       "          [0.6706, 0.6667, 0.6549,  ..., 0.4196, 0.4353, 0.4471],\n",
       "          [0.6824, 0.6745, 0.6824,  ..., 0.4118, 0.4275, 0.4392],\n",
       "          [0.6863, 0.6706, 0.6745,  ..., 0.4392, 0.4392, 0.4353]],\n",
       " \n",
       "         [[0.6510, 0.7176, 0.8118,  ..., 0.7765, 0.7725, 0.6588],\n",
       "          [0.5176, 0.5373, 0.6314,  ..., 0.8157, 0.7569, 0.5765],\n",
       "          [0.4118, 0.4510, 0.5647,  ..., 0.8000, 0.6824, 0.4824],\n",
       "          ...,\n",
       "          [0.6824, 0.6627, 0.6431,  ..., 0.3804, 0.4039, 0.4235],\n",
       "          [0.6941, 0.6667, 0.6667,  ..., 0.3569, 0.3765, 0.4039],\n",
       "          [0.6941, 0.6627, 0.6627,  ..., 0.3647, 0.3725, 0.3804]]])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross checking if the images is converted to tensors or not\n",
    "train_data['img'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# As now i have all the input data in the form of tensors, I will now convert the labels to tensors as well\n",
    "train_data['label'] = torch.nn.functional.one_hot(train_data['label'], num_classes=10).float()\n",
    "test_data['label'] = torch.nn.functional.one_hot(test_data['label'], num_classes=10).float()\n",
    "print(train_data['label'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "print(train_data['img'][0].shape) # Need to check the shape to calculate dimensions for other formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data is ready to be fed to the model\n",
    "# Now I will create a model\n",
    "\n",
    "class BeautifulModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BeautifulModel, self).__init__()\n",
    "        self.layer1 = torch.nn.Conv2d(3, 6, 5)\n",
    "        self.pool = torch.nn.MaxPool2d(2, 2) # Covering 2x2 window with a stride of 2\n",
    "        self.layer2 = torch.nn.Conv2d(6, 16, 5)\n",
    "        self.connecting_layer = torch.nn.Linear(16 * 5 * 5, 120)\n",
    "        self.connecting_layer2 = torch.nn.Linear(120, 84)\n",
    "        self.output_layer = torch.nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.nn.functional.relu(self.layer1(x)))\n",
    "        x = self.pool(torch.nn.functional.relu(self.layer2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = torch.nn.functional.relu(self.connecting_layer(x))\n",
    "        x = torch.nn.functional.relu(self.connecting_layer2(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BeautifulModel(\n",
       "  (layer1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (layer2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (connecting_layer): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (connecting_layer2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (output_layer): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BeautifulModel().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anshp\\AppData\\Local\\Temp\\ipykernel_13712\\1831480187.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress\n",
      "Epoch 0, Loss: 0.00020418466010596603, accuracy: 0.03125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anshp\\AppData\\Local\\Temp\\ipykernel_13712\\1831480187.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_labels = torch.tensor(test_labels).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Test Loss: 3.4087231159210205, accuracy: 0.437\n",
      "Training progress\n",
      "Epoch 1, Loss: 0.006442373152822256, accuracy: 0.03125\n",
      "Epoch 1, Test Loss: 3.1957385540008545, accuracy: 0.4397\n",
      "Training progress\n",
      "Epoch 2, Loss: 0.003602564102038741, accuracy: 0.03125\n",
      "Epoch 2, Test Loss: 2.908565044403076, accuracy: 0.4941\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(labels)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 15\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, labels)\n\u001b[0;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32md:\\python workspace\\Machine_Leaning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python workspace\\Machine_Leaning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[21], line 15\u001b[0m, in \u001b[0;36mBeautifulModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 15\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)))\n\u001b[0;32m     17\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# flatten all dimensions except batch\u001b[39;00m\n",
      "File \u001b[1;32md:\\python workspace\\Machine_Leaning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python workspace\\Machine_Leaning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\python workspace\\Machine_Leaning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:213\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python workspace\\Machine_Leaning\\.venv\\Lib\\site-packages\\torch\\_jit_internal.py:624\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\python workspace\\Machine_Leaning\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:830\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 830\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    for i in range(len(train_data['img'])):\n",
    "        inputs = train_data['img'][i:i+batch_size]\n",
    "        labels = train_data['label'][i:i+batch_size]\n",
    "\n",
    "        # Insuring that the input and labesl are tensors\n",
    "        inputs = torch.stack(inputs).to(device)\n",
    "        labels = torch.tensor(labels).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Training progress')\n",
    "    print(f'Epoch {epoch}, Loss: {loss.item()}, accuracy: {torch.sum(torch.argmax(output, dim=1) == torch.argmax(labels, dim=1)).item() / batch_size}')\n",
    "    with torch.no_grad():\n",
    "        test_inputs = test_data['img']\n",
    "        test_labels = test_data['label']\n",
    "        test_inputs = torch.stack(test_inputs).to(device)\n",
    "        test_labels = torch.tensor(test_labels).to(device)\n",
    "        test_output = model(test_inputs)\n",
    "        test_loss = criterion(test_output, test_labels)\n",
    "        print(f'Epoch {epoch}, Test Loss: {test_loss.item()}, accuracy: {torch.sum(torch.argmax(test_output, dim=1) == torch.argmax(test_labels, dim=1)).item() / len(test_data[\"img\"])}')\n",
    "print('Model has learned its stuff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
